> .parseGPLWithLimits <- function(con, n = NULL) {
+     txt <- vector("character")
+     i <- 0
+     hasDataTable = FALSE
+     while (i <- i + 1) {
+         tmp <- try(readLines(con, 1))
+         if (inherits(tmp, "try-error") || length(tmp) == 0 || (!is.null(n) && i ==
+                                                                n)) {
+             i <- i - 1  # counted an extra line
+             hasDataTable = FALSE
+             break
+         }
+         txt[i] <- tmp
+         if (i == length(txt))
+             txt <- c(txt, character(i))  # double vector size
+         if (length(grep("!\\w+_table_begin", txt[i], perl = TRUE)) > 0) {
+             hasDataTable = TRUE
+             break
+         }
+     }
+     txt <- txt[1:i]
+     cols <- parseGeoColumns(txt)
+     meta <- parseGeoMeta(txt)
+     geoDataTable <- new("GEODataTable", columns = data.frame(), table = data.frame())
+     if (hasDataTable) {
+         nLinesToRead <- NULL
+         if (!is.null(n)) {
+             nLinesToRead <- n - length(txt)
+         }
+         dat3 <- fastTabRead(con, n = nLinesToRead, quote = "")
+         geoDataTable <- new("GEODataTable", columns = cols, table = dat3[1:(nrow(dat3) -
+                                                                                 1), ])
+     }
+     gpl <- new("GPL", header = meta, dataTable = geoDataTable)
+ }
> 
> 
> #' @importFrom readr read_tsv
> .parseGSMTxt <- function(txt) {
+     parser_results = .genericGEOTableParser(txt)
+     
+     cols <- parseGeoColumns(parser_results$meta_text)
+     meta <- parseGeoMeta(parser_results$meta_text)
+     geoDataTable <- new("GEODataTable", columns = cols, table = parser_results$data_frame)
+     geo <- new("GSM", header = meta, dataTable = geoDataTable)
+     return(geo)
+ }
> 
> 
> #' @importFrom data.table fread
> parseGSM <- function(fname) {
+     txt = data.table::fread(fname, sep = "")[[1]]
+     # read_lines reads separate blank lines on windows, so remove them before
+     # proceeding. NOT doing so results in the Table header being repeated as the
+     # first line of the Table and test failures galore.
+     txt = txt[txt != ""]
+     return(.parseGSMTxt(txt))
+ }
> 
> ### In memory cache for GPL objects parsed from locally cached versions of GPL
> ### SOFT files.  It is disabled by default with
> ### options('GEOquery.inmemory.gpl'=FALSE).
> GPLcache <- new.env(parent = emptyenv())
> 
> 
> #' @importFrom readr read_tsv
> .parseGPLTxt <- function(txt) {
+     
+     parser_results = .genericGEOTableParser(txt)
+     
+     cols <- parseGeoColumns(parser_results$meta_text)
+     meta <- parseGeoMeta(parser_results$meta_text)
+     geoDataTable <- new("GEODataTable", columns = cols, table = parser_results$data_frame)
+     geo <- new("GPL", header = meta, dataTable = geoDataTable)
+     return(geo)
+ }
> 
> 
> #' @importFrom readr read_lines
> parseGPL <- function(fname) {
+     if (getOption("GEOquery.inmemory.gpl")) {
+         info <- file.info(fname, extra_cols = FALSE)
+         cache <- get0(fname, envir = GPLcache, inherits = FALSE)
+         ## Check if the locally cached version wasn't modified.
+         if (!is.null(cache) && cache$info$mtime == info$mtime) {
+             message("Using GPL object found in memory from locally cached version.")
+             return(cache$gpl)
+         }
+     }
+     txt = data.table::fread(fname, sep = "")[[1]]
+     # read_lines reads separate blank lines on windows, so remove them before
+     # proceeding. NOT doing so results in the Table header being repeated as the
+     # first line of the Table and test failures galore.
+     txt = txt[txt != ""]
+     return(.parseGPLTxt(txt))
+ }
> 
> 
> txtGrab <- function(regex, x) {
+     x <- as.character(x)
+     a <- regexpr(regex, x, perl = TRUE)
+     return(substr(x, a, a + attr(a, "match.length") - 1))
+ }
> 
> ### Function wrapper to get and parse ALL the GSEMatrix files associated with a
> ### GSE into a list of ExpressionSets
> getAndParseGSEMatrices <- function(GEO, destdir, AnnotGPL, getGPL = TRUE, parseCharacteristics = TRUE) {
+     GEO <- toupper(GEO)
+     ## This stuff functions to get the listing of available files for a given GSE
+     ## given that there may be many GSEMatrix files for a given GSE.
+     stub = gsub("\\d{1,3}$", "nnn", GEO, perl = TRUE)
+     gdsurl <- "https://ftp.ncbi.nlm.nih.gov/geo/series/%s/%s/matrix/"
+     b = getDirListing(sprintf(gdsurl, stub, GEO))
+     message(sprintf("Found %d file(s)", length(b)))
+     ret <- list()
+     ## Loop over the files, returning a list, one element for each file
+     for (i in 1:length(b)) {
+         message(b[i])
+         destfile = file.path(destdir, b[i])
+         if (file.exists(destfile)) {
+             message(sprintf("Using locally cached version: %s", destfile))
+         } else {
+             url = sprintf("https://ftp.ncbi.nlm.nih.gov/geo/series/%s/%s/matrix/%s",
+                           stub, GEO, b[i])
+             downloadFile(url, destfile = destfile, mode = "wb")
+         }
+         ret[[b[i]]] <- parseGSEMatrix(destfile, destdir = destdir, AnnotGPL = AnnotGPL,
+                                       getGPL = getGPL)$eset
+     }
+     return(ret)
+ }
> 
> #' Parse a GSE mstrix file
> #'
> #' Not meant for user calling, but parses
> #' a single GSEMatrix file.
> #'
> #' @importFrom dplyr select filter mutate mutate_all
> #' @importFrom tidyr gather spread separate
> #' @importFrom readr read_lines
> #' @importClassesFrom Biobase ExpressionSet
> #' @importFrom magrittr %>%
> #'
> #' @param fname filename
> #' @param AnnotGPL set to TRUE to get the annotation GPL version
> #' @param destdir the destdination directory for download
> #' @param getGPL whether or not to get the GPL associated
> #' @param parseCharacteristics Whether or not to do full 'characteristic' parsing
> #' @keywords internal
> #' 
> parseGSEMatrix <- function(fname, AnnotGPL = FALSE, destdir = tempdir(), getGPL = TRUE,
+                            parseCharacteristics = TRUE) {
+     # Convenient VERY fast line reader based on data.table See:
+     # https://stackoverflow.com/a/32924981/459633 data read into single character
+     # column, so subset to get just text.
+     dat <- data.table::fread(fname, sep = "")[[1]]
+     
+     ## get the number of !Series and !Sample lines
+     series_header_row_count <- sum(grepl("^!Series_", dat))
+     # In the case of ^M in the metadata (GSE781, for example), the line counts
+     # for 'skip' and read.table are different.  This next line gets the 'skip'
+     # line count for use below in the tmpdat reading 'skip'
+     sample_header_start <- grep("^!Sample_", dat)[1]
+     samples_header_row_count <- sum(grepl("^!Sample_", dat))
+     series_table_begin_line = grep("^!series_matrix_table_begin", dat)
+     series_table_end_line = grep("^!series_matrix_table_end", dat)
+     if (length(series_table_begin_line) != 1) {
+         stop("parsing failed--expected only one '!series_data_table_begin'")
+     }
+     # con <- fileOpen(fname) Read the !Series_ and !Sample_ lines
+     header <- data.table::fread(fname, header = FALSE, nrows = series_header_row_count)
+     tmpdat <- data.table::fread(fname, header = FALSE, nrows = samples_header_row_count,
+                                 skip = sample_header_start - 1)
+     
+     headertmp <- t(header)
+     headerdata <- rbind(data.frame(), headertmp[-1, ])
+     colnames(headerdata) <- sub("!Series_", "", as.character(header[[1]]))
+     headerlist <- lapply(split.default(headerdata, names(headerdata)), function(x) {
+         as.character(Reduce(function(a, b) {
+             paste(a, b, sep = "\n")
+         }, x))
+     })
+     
+     link = "https://www.ncbi.nlm.nih.gov/geo/"
+     if (!is.null(headerlist$web_link)) {
+         link <- headerlist$web_link
+     } else if (!is.null(headerlist$geo_accession)) {
+         link <- paste(link, "query/acc.cgi?acc=", headerlist$geo_accession, sep = "")
+     }
+     
+     ed <- new("MIAME", name = ifelse(is.null(headerlist$contact_name), "", headerlist$contact_name),
+               title = headerlist$title, contact = ifelse(is.null(headerlist$contact_email),
+                                                          "", headerlist$contact_email), pubMedIds = ifelse(is.null(headerlist$pubmed_id),
+                                                                                                            "", headerlist$pubmed_id), abstract = ifelse(is.null(headerlist$summary),
+                                                                                                                                                         "", headerlist$summary), url = link, other = headerlist)
+     
+     tmptmp <- t(tmpdat)
+     sampledat <- rbind(data.frame(), tmptmp[-1, ])
+     colnames(sampledat) <- make.unique(sub("!Sample_", "", as.character(tmpdat[[1]])))
+     sampledat[["geo_accession"]] = as.character(sampledat[["geo_accession"]])
+     rownames(sampledat) = sampledat[["geo_accession"]]
+     ## Lots of GSEs now use 'characteristics_ch1' and 'characteristics_ch2' for
+     ## key-value pairs of annotation. If that is the case, this simply cleans
+     ## those up and transforms the keys to column names and the values to column
+     ## values.
+     if (length(grep("characteristics_ch", colnames(sampledat))) > 0 && parseCharacteristics) {
+         pd = sampledat %>%
+             dplyr::select(dplyr::contains("characteristics_ch")) %>%
+             dplyr::mutate(accession = rownames(.)) %>%
+             # these next two lines avoid warnings due to columns having different
+             # factor levels (attributes).
+             mutate_all(as.character) %>%
+             tidyr::gather(characteristics, kvpair, -accession) %>%
+             dplyr::filter(grepl(":", kvpair) & !is.na(kvpair))
+         # Thx to Mike Smith (@grimbough) for this code sometimes the
+         # 'characteristics_ch1' fields are empty and contain no key:value pairs.
+         # spread() will fail when called on an empty data_frame.  We catch this
+         # case and remove the 'charactics_ch1' column instead
+         if (nrow(pd)) {
+             pd = dplyr::mutate(pd, characteristics = ifelse(grepl("_ch2", characteristics),
+                                                             "ch2", "ch1")) %>%
+                 tidyr::separate(kvpair, into = c("k", "v"), sep = ":", fill = "right",
+                                 extra = "merge") %>%
+                 dplyr::mutate(k = paste(k, characteristics, sep = ":")) %>%
+                 dplyr::select(-characteristics) %>%
+                 dplyr::filter(!is.na(v)) %>%
+                 dplyr::group_by(accession, k) %>%
+                 dplyr::mutate(v = paste0(trimws(v), collapse = ";")) %>%
+                 unique() %>%
+                 tidyr::spread(k, v)
+         } else {
+             pd = pd %>%
+                 dplyr::select(accession)
+         }
+         ## dplyr::mutate(characteristics=ifelse(grepl('_ch2',characteristics),'ch2','ch1'))
+         ## %>% dplyr::filter(grepl(':',kvpair)) %>% tidyr::separate(kvpair, into=
+         ## c('k','v'), sep=':') if(nrow(pd)>0) { pd = pd %>% dplyr::mutate(k =
+         ## paste(k,characteristics,sep=':')) %>% dplyr::select(-characteristics)
+         ## %>% tidyr::spread(k,v)
+         sampledat = sampledat %>%
+             dplyr::left_join(pd, by = c(geo_accession = "accession"))
+     }
+     
+     datamat = NULL
+     if (series_table_end_line - series_table_begin_line == 2) {
+         datamat = read.table(textConnection(dat[(series_table_begin_line + 1):(series_table_end_line -
+                                                                                    1)]), header = TRUE, sep = "\t")
+     } else {
+         datamat <- data.table::fread(text = dat[(series_table_begin_line + 1):(series_table_end_line -
+                                                                                    1)], quote = "\"", na.strings = c("NA", "null", "NULL", "Null"))
+     }
+     ## kip = series_table_begin_line) comment.char = '!series_matrix_table_end')
+     tmprownames = as.character(datamat[[1]])
+     # need the as.matrix for single-sample or empty GSE
+     datamat <- as.matrix(datamat[!is.na(tmprownames), -1])
+     rownames(datamat) <- tmprownames[!is.na(tmprownames)]
+     datamat <- as.matrix(datamat)
+     rownames(sampledat) <- colnames(datamat)
+     GPL = as.character(sampledat[1, grep("^platform_id", colnames(sampledat), ignore.case = TRUE)])
+     ## if getGPL is FALSE, skip this and featureData is then a data.frame with no
+     ## columns
+     fd = new("AnnotatedDataFrame", data = data.frame(row.names = rownames(datamat)))
+     if (getGPL) {
+         gpl <- getGEO(GPL, AnnotGPL = AnnotGPL, destdir = destdir)
+         vmd <- Columns(gpl)
+         dat <- Table(gpl)
+         ## GEO uses 'TAG' instead of 'ID' for SAGE GSE/GPL entries, but it is
+         ## apparently always the first column, so use dat[,1] instead of dat$ID
+         ## The next line deals with the empty GSE
+         tmpnames = character(0)
+         if (ncol(dat) > 0) {
+             tmpnames = as.character(dat[, 1])
+         }
+         ## Fixed bug caused by an ID being 'NA' in GSE15197, for example
+         tmpnames[is.na(tmpnames)] = "NA"
+         rownames(dat) <- make.unique(tmpnames)
+         ## Apparently, NCBI GEO uses case-insensitive matching between platform
+         ## IDs and series ID Refs ???
+         dat <- dat[match(tolower(rownames(datamat)), tolower(rownames(dat))), ]
+         # Fix possibility of duplicate column names in the GPL files; this is
+         # prevalent in the Annotation GPLs
+         rownames(vmd) <- make.unique(colnames(Table(gpl)))
+         colnames(dat) <- rownames(vmd)
+         fd <- new("AnnotatedDataFrame", data = dat, varMetadata = vmd)
+     }
+     if (is.null(nrow(datamat))) {
+         ## fix empty GSE datamatrix samplename stuff above does not work with
+         ## empty GSEs, so fix here, also
+         tmpnames <- names(datamat)
+         rownames(sampledat) <- tmpnames
+         datamat = matrix(nrow = 0, ncol = nrow(sampledat))
+         colnames(datamat) <- tmpnames
+     } else {
+         ## This looks like a dangerous operation but is needed to deal with the
+         ## fact that NCBI GEO allows case-insensitive matching and we need to
+         ## pick one.
+         rownames(datamat) <- rownames(dat)
+     }
+     eset <- new("ExpressionSet", phenoData = as(sampledat, "AnnotatedDataFrame"), annotation = GPL,
+                 featureData = fd, experimentData = ed, exprs = as.matrix(datamat))
+     return(list(GPL = as.character(sampledat[1, grep("platform_id", colnames(sampledat),
+                                                      ignore.case = TRUE)]), eset = eset))
+ }
> 
> 
> 
> #' get all download links from a GEO accession
> #'
> #' This function gets all download links from a GEO accession number.
> #'
> #' @param gse GEO accession number
> #'
> #' @return A character vector with all download links
> #'
> #' @keywords internal
> getGSEDownloadPageURLs <- function(gse) {
+     url <- "https://ncbi.nlm.nih.gov/geo/download/"
+     links <- httr2::request(url) |>
+         httr2::req_timeout(15) |>
+         httr2::req_retry(3) |>
+         httr2::req_url_query(acc = gse) |>
+         httr2::req_perform() |>
+         httr2::resp_body_string() |>
+         rvest::read_html() |>
+         rvest::html_nodes("a") |>
+         rvest::html_attr("href") |>
+         stringr::str_replace("^/geo/", "https://www.ncbi.nlm.nih.gov/geo/") |>
+         stringr::str_replace("^ftp://", "https://")
+     class(links) <- c("geoDownloadLinks", class(links))
+     return(links)
+ }
> 
> 
> #' Get the link to the raw counts file from GEO
> #'
> #' This function extracts the link to the raw counts file from a
> #' geoDownloadLinks object.
> #'
> #' @param links A geoDownloadLinks object
> #'
> #' @return A character vector with the link to the raw counts file
> #'
> #' @keywords internal
> getRNAQuantRawCountsURL <- function(links) {
+     if (!inherits(links, "geoDownloadLinks")) {
+         stop("Input must be a geoDownloadLinks object")
+     }
+     link <- stringr::str_subset(links, "raw_counts")
+     return(link)
+ }
> 
> #' Get the RNA-seq quantification annotation link
> #'
> #' This function extracts the link to the RNA-seq quantification annotation
> #' file from a geoDownloadLinks object.
> #'
> #' @param links A geoDownloadLinks object
> #'
> #' @return A character vector with the link to the annotation file
> #'
> #' @keywords internal
> getRNAQuantAnnotationURL <- function(links) {
+     if (!inherits(links, "geoDownloadLinks")) {
+         stop("Input must be a geoDownloadLinks object")
+     }
+     link <- stringr::str_subset(links, "annot.tsv.gz")
+     return(link)
+ }
> 
> #' Extract filename from a GEO download URL
> #'
> #' This function extracts the filename from a GEO download URL.
> #' The filename is expected to be a query parameter called "file".
> #' If the query parameter is not found, the function returns NULL.
> #'
> #' The idea is to use this function to extract filenames that
> #' contain important metadata from the GEO RNA-seq quantification.
> #'
> #' In particular, the filename is expected to contain the genome build
> #' and species information that we can attach to the SummarizedExperiment.
> #'
> #' @param url A GEO download URL
> #'
> #' @return A character vector with the filename
> #'
> #' @keywords internal
> extractFilenameFromDownloadURL <- function(url) {
+     # get a query parameter called "file" and its value from
+     # the URL
+     
+     # example URL: https://www.ncbi.nlm.nih.gov/geo/download/\
+     # ?format=file&type=rnaseq_counts&file=Human.GRCh38.p13.annot.tsv.gz
+     parsed <- httr2::url_parse(url)
+     fname <- NULL
+     if ("query" %in% names(parsed)) {
+         if ("file" %in% names(parsed$query)) {
+             fname <- parsed$query$file
+         }
+     }
+     fname
+ }
> 
> #' Extract genome build and species from a GEO download URL
> #'
> #' This function extracts the genome build and species information
> #' from a GEO download URL. The genome build and species information
> #' is expected to be in the filename of the download URL.
> #'
> #' @param url A GEO annotation file download URL
> #'
> #' @return A character vector with the genome build and species information
> #'
> #' @keywords internal
> urlExtractRNASeqQuantGenomeInfo <- function(url) {
+     fname <- extractFilenameFromDownloadURL(url)
+     if (is.null(fname)) {
+         return(NULL)
+     }
+     splits <- stringr::str_split(fname, "\\.")[[1]]
+     genome_build <- paste0(splits[2], ".", splits[3])
+     species <- splits[1]
+     return(c(genome_build = genome_build, species = species, fname = fname))
+ }
> 
> #' Extract genome build and species for GEO RNA-seq quantification
> #'
> #' This function extracts the genome build and species information
> #' for a GEO RNA-seq quantification.
> #'
> #' @param gse GEO accession number
> #'
> #' @return A character vector with the genome build and species information
> #'
> #' @examples
> #' getRNASeqQuantGenomeInfo("GSE164073")
> #'
> #' @export
> getRNASeqQuantGenomeInfo <- function(gse) {
+     links <- getGSEDownloadPageURLs(gse)
+     annotation_link <- getRNAQuantAnnotationURL(links)
+     metadata <- urlExtractRNASeqQuantGenomeInfo(annotation_link)
+     return(metadata)
+ }
> 
> 
> #' Read RNA-seq quantification annotation from GEO
> #'
> #' This function reads the annotation file from a GEO link. The annotation
> #' file is expected to be a tab-separated file with the first column
> #' containing the gene IDs and the remaining columns containing the
> #' annotation information.
> #'
> #' @param link A link to the annotation file
> #'
> #' @return A data frame of annotation information with gene IDs as row names
> #'
> #' @keywords internal
> readRNAQuantAnnotation <- function(link) {
+     annotation <- as.data.frame(readr::read_tsv(link, show_col_types = FALSE))
+     rownames(annotation) <- as.character(annotation$GeneID)
+     return(annotation)
+ }
> 
> 
> #' Read raw counts from GEO
> #'
> #' This function reads the raw counts from a GEO link. The raw counts
> #' are expected to be in a tab-separated file with the first column
> #' containing the gene IDs and the remaining columns containing the
> #' raw counts.
> #'
> #' This function reads the raw counts and returns a matrix with the
> #' gene IDs as the row names, ready for use in creating a
> #' SummarizedExperiment.
> #'
> #' @param link A link to the raw counts file
> #'
> #' @return A matrix of raw counts with gene IDs as row names
> #'
> #' @keywords internal
> readRNAQuantRawCounts <- function(link) {
+     quants <- readr::read_tsv(link, show_col_types = FALSE)
+     gene_ids <- as.character(quants$GeneID)
+     quants <- as.matrix(quants[, -1])
+     rownames(quants) <- gene_ids
+     return(quants)
+ }
> 
> 
> #' Get RNA-seq quantification and annotation from GEO
> #'
> #' This function downloads the raw counts and annotation files from GEO
> #' for a given GEO accession number.
> #'
> #' @param gse GEO accession number
> #'
> #' @return A list with two elements: quants (a matrix of raw counts) and
> #' annotation (a data frame of annotation information).
> #'
> #' @keywords internal
> getRNASeqQuantResults <- function(gse) {
+     links <- getGSEDownloadPageURLs(gse)
+     raw_counts_link <- getRNAQuantRawCountsURL(links)
+     if (length(raw_counts_link) == 0) {
+         stop(
+             "No raw counts file found.\n",
+             "Navigate to: \n  https://ncbi.nlm.nih.gov/geo/download/?acc=",
+             gse,
+             "\nand check if the 'RNA-Seq raw counts' link is available."
+         )
+     }
+     annotation_link <- getRNAQuantAnnotationURL(links)
+     quants <- readRNAQuantRawCounts(raw_counts_link)
+     annotation <- readRNAQuantAnnotation(annotation_link)
+     return(list(quants = quants, annotation = annotation))
+ }
> 
> #' Browse GEO search website for RNA-seq datasets
> #'
> #' This function opens a browser window to the NCBI GEO website
> #' with a search for RNA-seq datasets. It is included as a convenience
> #' function to remind users of how to search for RNA-seq datasets using
> #' the NCBI GEO website and an "rnaseq counts" filter.
> #'
> #' @examples
> #' \dontrun{
> #' browseWebsiteRNASeqSearch()
> #' }
> #' @export
> browseWebsiteRNASeqSearch <- function() {
+     browseURL(
+         "https://ncbi.nlm.nih.gov/gds?term=%22rnaseq%20counts%22%5BFilter%5D"
+     )
+ }
> 
> 
> #' Does a GEO accession have RNA-seq quantifications?
> #'
> #' This function checks if a GEO accession number has RNA-seq quantifications
> #' available. It does this by checking if the GEO accession number has a
> #' "RNA-Seq raw counts" link available on the GEO download page.
> #'
> #' @param accession GEO accession number
> #'
> #' @return TRUE if the GEO accession number has RNA-seq quantifications
> #' available, FALSE otherwise.
> #'
> #' @examples
> #' hasRNASeqQuantifications("GSE164073")
> #'
> #' @export
> hasRNASeqQuantifications <- function(accession) {
+     links <- getGSEDownloadPageURLs(accession)
+     raw_counts_link <- getRNAQuantRawCountsURL(links)
+     if (length(raw_counts_link) > 0) {
+         return(TRUE)
+     } else {
+         return(FALSE)
+     }
+ }
> 
> #' Get GEO RNA-seq quantifications as a SummarizedExperiment object
> #'
> #' For human and mouse GEO datasets, NCBI GEO attempts to process
> #' the raw data and provide quantifications in the form of raw counts
> #' and an annotation file. This function downloads the raw counts and
> #' annotation files from GEO and merges that with the metadata from the GEO
> #' object to create a SummarizedExperiment.
> #'
> #' @details
> #' A major barrier to fully exploiting and reanalyzing the massive volumes
> #' of public RNA-seq data archived by SRA is the cost and effort required to
> #' consistently process raw RNA-seq reads into concise formats that summarize
> #' the expression results. To help address this need, the NCBI SRA and GEO
> #' teams have built a pipeline that precomputes RNA-seq gene expression counts
> #' and delivers them as count matrices that may be incorporated into commonly
> #' used differential expression analysis and visualization software.
> #'
> #' The pipeline processes RNA-seq data from SRA using the HISAT2 aligner and
> #' and then generates gene expression counts using the featureCounts program.
> #'
> #' See the
> #' [GEO documentation](https://ncbi.nlm.nih.gov/geo/info/rnaseqcounts.html)
> #' for more details.
> #'
> #' @import SummarizedExperiment
> #'
> #'
> #' @param accession GEO accession number
> #'
> #' @return A SummarizedExperiment object with the raw counts as the counts
> #' assay, the annotation as the rowData, and the metadata from GEO as
> #' the colData.
> #'
> #'
> #' @examples
> #' se <- getRNASeqData("GSE164073")
> #' se
> #'
> #' @export
> getRNASeqData <- function(accession) {
+     quantifications <- getRNASeqQuantResults(accession)
+     se <- as(
+         getGEO(accession)[[1]],
+         "SummarizedExperiment"
+     )
+     old_metadata <- S4Vectors::metadata(se)
+     old_metadata$genomeInfo <- getRNASeqQuantGenomeInfo(accession)
+     old_metadata$created_at <- Sys.time()
+     new_se <- SummarizedExperiment::SummarizedExperiment(
+         assays = list(counts = quantifications$quants),
+         rowData = quantifications$annotation,
+         colData = SummarizedExperiment::colData(se),
+         metadata = old_metadata
+     )
+     new_se
+ }
> 
> 
> 
> #' Search GEO database
> #'
> #' This function searches the [GDS](https://www.ncbi.nlm.nih.gov/gds)
> #' database, and return a data.frame for all the search results.
> #'
> #' The NCBI allows users to access more records (10 per second) if they register
> #' for and use an API key. [set_entrez_key][rentrez::set_entrez_key] function
> #' allows users to set this key for all calls to rentrez functions during a
> #' particular R session. You can also set an environment variable `ENTREZ_KEY`
> #' by [Sys.setenv][base::Sys.setenv].  Once this value is set to your key
> #' rentrez will use it for all requests to the NCBI. Details see
> #' <https://docs.ropensci.org/rentrez/articles/rentrez_tutorial.html#rate-limiting-and-api-keys>
> #'
> #' @param query character, the search term. The NCBI uses a search term syntax
> #' which can be associated with a specific search field with square brackets.
> #' So, for instance "Homo sapiens\[ORGN\]" denotes a search for `Homo sapiens`
> #' in the “Organism” field. Details see
> #' <https://www.ncbi.nlm.nih.gov/geo/info/qqtutorial.html>. The names and
> #' definitions of these fields can be identified using
> #' [searchFieldsGEO].
> #'
> #' @seealso [searchFieldsGEO]
> #'
> #' @param step the number of records to fetch from the database each time. You
> #' may choose a smaller value if failed.
> #'
> #' @return a data.frame contains the search results
> #'
> #' @examples
> #' \dontrun{
> #' searchGEO("diabetes[ALL] AND Homo sapiens[ORGN] AND GSE[ETYP]")
> #' }
> #'
> #' @export
> searchGEO <- function(query, step = 500L) {
+     records_num <- rentrez::entrez_search(
+         "gds", query,
+         retmax = 0L
+     )$count
+     seq_starts <- seq(1L, records_num, step)
+     records <- character(length(seq_starts))
+     search_res <- rentrez::entrez_search(
+         "gds", query,
+         use_history = TRUE, retmax = 0L
+     )
+     for (i in seq_along(seq_starts)) {
+         records[[i]] <- rentrez::entrez_fetch(
+             db = "gds", web_history = search_res$web_history,
+             rettype = "summary", retmode = "text",
+             retmax = step, retstart = seq_starts[[i]]
+         )
+         Sys.sleep(1L)
+     }
+     records <- strsplit(
+         gsub("^\\n|\\n$", "", paste0(records, collapse = "")),
+         "\\n\\n"
+     )[[1L]]
+     name_value_pairs <- parse_name_value_pairs(preprocess_records(records))
+     data.table::setDF(name_value_pairs)
+     name_value_pairs
+ }
> 
> 
> #' Provide a list of possible search fields for GEO search
> #'
> #' @import rentrez
> #'
> #' @returns a data.frame with names of possible search fields for GEO search
> #' as well as descriptions, data types, etc. for each field. Fields are
> #' in rows and their properties are in columns.
> #'
> #' @seealso \code{\link{searchGEO}}
> #'
> #' @examples
> #' searchFieldsGEO()
> #'
> #' @export
> searchFieldsGEO <- function() {
+     res <- do.call(
+         rbind,
+         rentrez::entrez_db_searchable("gds")
+     ) |> data.frame()
+     rownames(res) <- NULL
+     res
+ }
> 
> 
> 
> # this function just processed GEO searched results returned by `entrez_fetch`
> # into key-values paris
> preprocess_records <- function(x) {
+     x <- sub("^\\d+\\.", "Title:", x, perl = TRUE)
+     x <- sub(
+         "\\n\\(Submitter supplied\\)\\s*",
+         "\nSummary: ", x,
+         perl = TRUE
+     )
+     x <- gsub(
+         "\\s*\\n?(Platform|Dataset)s?\\s*:\\s*",
+         "\n\\1s: ", x,
+         perl = TRUE
+     )
+     x <- sub("\\tID:\\s*", "\nID: ", x, perl = TRUE)
+     x <- sub(
+         "\\n?\\s*((?:\\s*\\d+(?:\\s*related)?\\s*(?:DataSet|Platform|Sample|Serie)s?)+)([^:])",
+         "\nContains: \\1\\2", x,
+         perl = TRUE
+     )
+     x <- gsub(":\\t+", ": ", x, perl = TRUE)
+     x <- gsub("\\t\\t+", " ", x, perl = TRUE)
+     strsplit(x, "\\n", perl = TRUE)
+ }
> 
> # parse key-value pairs separeted by ":". For a list of key-value pairs
> # characters (like: `list(c("a:1", "b:2"), c("a:3", "b:4"))`), this function
> # simply cleans those up and transforms the list into a list object, the names
> # of returned value is the unique keys in the pairs, the element of the returned
> # list is the values in the paris.
> # See parse_name_value_pairs(list(c("a:1", "b:2"), c("a:3", "b:4")))
> #' @return a list, every element of which corresponds to each key-value pairs
> #' group by key in the paris.
> #' @noRd
> parse_name_value_pairs <- function(chr_list, sep = ":") {
+     .characteristic_list <- lapply(chr_list, function(x) {
+         if (!length(x)) {
+             return(data.table::data.table())
+         }
+         # Don't use `data.table::tstrsplit`, as it will split string into three
+         # or more elements.
+         name_value_pairs <- data.table::transpose(
+             str_split(x, paste0("(\\s*+)", sep, "(\\s*+)"))
+         )
+         res <- as.list(name_value_pairs[[2L]])
+         names(res) <- name_value_pairs[[1L]]
+         data.table::setDT(res)
+         res
+     })
+     characteristic_dt <- data.table::rbindlist(
+         .characteristic_list,
+         use.names = TRUE, fill = TRUE
+     )
+     data.table::setnames(characteristic_dt, make.unique)
+     
+     # parse text into corresponding atomic vector mode
+     lapply(characteristic_dt, function(x) {
+         data.table::fread(
+             text = x, sep = "", header = FALSE,
+             strip.white = TRUE, blank.lines.skip = FALSE, fill = TRUE
+         )[[1L]]
+     })
+ }
> 
> # split string based on pattern, Only split once, Return a list of character,
> # the length of every element is two
> str_split <- function(string, pattern, ignore.case = FALSE) {
+     regmatches(
+         string,
+         regexpr(pattern, string,
+                 perl = TRUE, fixed = FALSE,
+                 ignore.case = ignore.case
+         ),
+         invert = TRUE
+     )
+ }
> 
> 
> .onLoad <- function(lib, pkg) {
+     packageStartupMessage("Setting options('download.file.method.GEOquery'='auto')")
+     options(download.file.method.GEOquery = "auto")
+     packageStartupMessage("Setting options('GEOquery.inmemory.gpl'=FALSE)")
+     options(GEOquery.inmemory.gpl = FALSE)
+ }
> Version: 1.0
Hata: 'Version' nesnesi bulunamadı
> # build geo docsum jsons
> res = searchGEODocSums()
Error in searchGEODocSums() : "searchGEODocSums" fonksiyonu bulunamadı
> require(rentrez)
Zorunlu paket yükleniyor: rentrez
Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘rentrez’
> require(purrr)
Zorunlu paket yükleniyor: purrr
> require(lubridate)
Zorunlu paket yükleniyor: lubridate

Attaching package: ‘lubridate’

The following objects are masked from ‘package:base’:

    date, intersect, setdiff, union

Warning message:
package ‘lubridate’ was built under R version 4.3.3 
> 
> .docSumListConvert = function(x) {
+     ret = data.frame(
+         uid          = x %>% purrr::map('uid') %>% purrr::flatten_chr() %>% as.integer(),
+         accession    = x %>% purrr::map('accession') %>% purrr::flatten_chr(),
+         seriestitle  = x %>% purrr::map('seriestitle') %>% purrr::flatten_chr(),
+         platformtitle= x %>% purrr::map('platformtitle') %>% purrr::flatten_chr(),
+         taxon        = x %>% purrr::map('gpl') %>% purrr::map(strsplit,';') %>% purrr::flatten() %>% I(),
+         entrytype    = x %>% purrr::map('entrytype') %>% purrr::flatten_chr(),
+         gdstype      = x %>% purrr::map('gdstype') %>% purrr::flatten_chr(),
+         ptechtype    = x %>% purrr::map('ptechtype') %>% purrr::flatten_chr(),
+         valtype      = x %>% purrr::map('valtype') %>% purrr::flatten_chr(),
+         ssinfo       = x %>% purrr::map('ssinfo') %>% purrr::map(strsplit,';') %>% purrr::flatten() %>% I(),
+         title        = x %>% purrr::map('title') %>% purrr::flatten_chr(),
+         summary      = x %>% purrr::map('summary') %>% purrr::flatten_chr(),
+         gpl          = x %>% purrr::map('gpl') %>% purrr::map(strsplit,';') %>% purrr::flatten() %>% purrr::map(function(x) paste0('GPL',x)) %>% I(),
+         gse          = x %>% purrr::map('gse') %>% purrr::map(strsplit,';') %>% purrr::flatten() %>% purrr::map(function(x) paste0('GSE',x)) %>% I(),
+         gds          = x %>% purrr::map('gds') %>% purrr::map(strsplit,';') %>% purrr::flatten() %>% purrr::map(function(x) ifelse(nchar(x)>0,paste0('GDS',x),"")) %>% I(),
+         samples      = x %>% purrr::map('samples') %>% purrr::map('accession') %>% I(),
+         suppfile     = x %>% purrr::map('suppfile') %>% purrr::map(strsplit,', ') %>% purrr::flatten() %>% I(),
+         ftplink      = x %>% purrr::map('ftplink') %>% purrr::flatten_chr(),
+         n_samples    = x %>% purrr::map('n_samples') %>% as.integer(),
+         pubmedids    = x %>% purrr::map('pubmedids') %>% I(),
+         projects     = x %>% purrr::map('projects') %>% I(),
+         public_date  = x %>% purrr::map('pdat') %>% purrr::flatten_chr() %>% lubridate::date(),
+         samplestaxa  = x %>% purrr::map('samplestaxa') %>% I() #purrr::map(strsplit,', ') %>% purrr::flatten() %>% I()
+     )
+     return(ret)
+ }
> 
> .docSumListFix = function(x) {
+     if(!is.list(x)) x = list(x)
+     ret = lapply(x,function(val) {
+         val$gpl = list(paste0('GPL',stringi::stri_split(val$gpl,fixed=';')[[1]]))
+         val$gse = list(paste0('GSE',stringi::stri_split(val$gse,fixed=';')[[1]]))
+         val$gds = list(paste0('GDS',stringi::stri_split(val$gds,fixed=';')[[1]]))
+         val$ssinfo  = unique(stringi::stri_split(val$ssinfo,fixed=';')[[1]])
+         return(val)
+     })
+     return(ret)
+ }
> 
> #' get all GEO records as docsums
> #'
> #' Fetches all records from NCBI entrez and creates a data frame
> #' from the docsum entries, loadable into a SQL-like database
> #'
> #'
> searchGEODocSums = function(term='GPL[ETYP] OR GSE[ETYP] OR GSM[ETYP] OR GDS[ETYP]') {
+     res = rentrez::entrez_search(db='gds',use_history=TRUE,term=term)
+     return(res)
+ }
> fetchDocSums= function(res,retstart=1,retmax=100) {
+     z=NULL
+     while(is.null(z) || inherits(z,'error')) {
+         z = try(rentrez::entrez_summary(db='gds',
+                                         web_history=res$web_history,
+                                         retstart=retstart,
+                                         retmax=retmax))
+     }
+     
+     return(.docSumListConvert(z))
+ }
> .bqSchemaFromDocsums = function(exampleDF) {
+     mappings = list(integer=list(mode='nullable',type='integer'),
+                     factor =list(mode="nullable",type="string"),
+                     character = list(mode='nullable',type='string'),
+                     numeric = list(mode="nullable",type="float"),
+                     logical = list(mode="nullable",type="boolean"),
+                     AsIs    = list(mode="repeated",type="string"),
+                     Date    = list(mode="nullable",type="string"))
+     coltypes = sapply(exampleDF,class)
+     coldesc  = mappings[coltypes]
+     names(coldesc)=NULL
+     coldesc=lapply(seq_along(coldesc),function(col) {
+         ret = coldesc[[col]]
+         ret$name=colnames(exampleDF)[col]
+         return(ret)
+     })
+     return(coldesc)
+ }
> 
> 
> getGEOMeta = function(geo) {
+     .getGSMmeta = function(x) {
+         ret = list(
+             accession     = xml_attr(xml_find_first(dat,'/d1:MINiML/d1:Sample'),'iid'),
+             gpl           = xml_attr(xml_find_first(dat,'/d1:MINiML/d1:Platform'),'iid'),
+             title         = xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Title')),
+             description   = xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Description')),
+             type          = xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Type')),
+             submission_date= xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Status/d1:Submission-Date')),
+             last_update   = xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Status/d1:Last-Update--Date')),
+             release_date  = xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Status/d1:Release-Date')),
+             n_channels    = as.integer(xml_text(xml_find_first(dat,'/d1:MINiML/d1:Sample/d1:Channel-Count'))),
+             channels      = data.frame(do.call(rbind,lapply(xml_find_all(dat,'/d1:MINiML/d1:Sample/d1:Channel'),
+                                                             function(channel) {
+                                                                 return(list(
+                                                                     tax_id = xml_attr(xml_find_first(channel,'./d1:Organism'),'taxid'),
+                                                                     organism = xml_text(xml_find_first(channel,'./d1:Organism')),
+                                                                     source = xml_text(xml_find_first(channel,'./d1:Source')),
+                                                                     molecule = xml_text(xml_find_first(channel,'./d1:Molecule')),
+                                                                     treatment_protocol = xml_text(xml_find_first(channel,'./d1:Treatment-Protocol')),
+                                                                     extract_protocol = str_trim(xml_text(xml_find_first(channel,'./d1:Extract-Protocol'))),
+                                                                     label_protocol = xml_text(xml_find_first(channel,'./d1:Label-Protocol')),
+                                                                     scan_protocol = xml_text(xml_find_first(channel,'./d1:Scan-Protocol')),
+                                                                     hybridization_protocol = xml_text(xml_find_first(channel,'./d1:Hybridization-Protocol')),
+                                                                     growth_protocol = str_trim(xml_text(xml_find_first(channel,'./d1:Growth-Protocol'))),
+                                                                     characteristics = sapply(xml_find_all(channel,'d1:Characteristics'),function(x) {str_trim(xml_text(x))}) %>%
+                                                                         setNames(sapply(xml_find_all(channel,'d1:Characteristics'),function(x) {str_trim(xml_attrs(x,'tag'))})) %>% I()
+                                                                 ))
+                                                             }))),
+             columns       = data.frame(do.call(rbind,lapply(xml_find_all(dat,'/d1:MINiML/d1:Sample/d1:Data-Table/d1:Column'),
+                                                             function(column) {
+                                                                 return(list(
+                                                                     name = xml_text(xml_find_first(column,'d1:Name')),
+                                                                     description = xml_text(xml_find_first(column,'d1:Description'))
+                                                                     
+                                                                 ))
+                                                             })))
+             
+         )
+         return(ret)
+     }
+     geo = toupper(geo)
+     stopifnot(substr(geo,1,3) %in% c('GSE','GSM','GPL','GDS'))
+     
+     dat = read_xml(sprintf("https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=%s&view=full&targ=self&form=xml",geo))
+     ret = switch(substr(geo,1,3),
+                  GSM = .getGSMmeta(dat),
+                  NA)
+     return(ret)
+ }
> 
> 
> 
> # build geo docsum jsons
> res = searchGEODocSums()
Error in loadNamespace(x) : there is no package called ‘rentrez’
> # build geo docsum jsons
> res = searchGEODocSums()
Error in loadNamespace(x) : there is no package called ‘rentrez’
Called from: doWithOneRestart(return(expr), restart)
Browse[1]> 
> library(testthat)

Attaching package: ‘testthat’

The following object is masked from ‘package:purrr’:

    is_null

Warning message:
package ‘testthat’ was built under R version 4.3.3 
> library(limma)

Attaching package: ‘limma’

The following object is masked _by_ ‘.GlobalEnv’:

    printHead

> context('GEO conversions')
> 
> test_that("GDS2eSet works", {
+     gds = getGEO(filename=system.file("extdata/GDS507.soft.gz",package="GEOquery"))
+     
+     eset = GDS2eSet(gds)
+     
+     expect_is(eset,'ExpressionSet') # eset should be an ExpressionSet
+     expect_equivalent(pubMedIds(experimentData(eset)),'14641932') #basic experimentData check failed
+     expect_equivalent(nrow(eset),22645) #'eset has wrong number of rows')
+     expect_equivalent(ncol(eset),17) #'eset has wrong number of columns')
+ })
── Error: GDS2eSet works ────────────────────────────────────────────────────────────────────────────────────
Error in `getClass(Class, where = topenv(parent.frame()))`: b
Backtrace:
    ▆
 1. └─global GDS2eSet(gds)
 2.   └─methods::new("AnnotatedDataFrame", data = tmp)
 3.     └─methods::getClass(Class, where = topenv(parent.frame()))

Error:
! Test failed
Run `rlang::last_trace()` to see where the error occurred.
